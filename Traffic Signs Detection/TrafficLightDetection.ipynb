{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8b9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e2a8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./datasets/traffic-signs-preprocessed/label_names.csv')\n",
    "with open('./datasets/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c35b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(labels.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57256c1",
   "metadata": {},
   "source": [
    "# Loading models\n",
    "## YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98c528b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_cfg = './YOLOV5/yolov5,runs/train/exp/weights'\n",
    "path_to_weights = './YOLOV5/yolov5/runs/train/exp2/weights/best.onnx'\n",
    "w = \"./best.onnx\"\n",
    "\n",
    "#network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
    "network = cv2.dnn.readNetFromONNX(path_to_weights)\n",
    "# To use with GPU\n",
    "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86c53d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layers_all = network.getLayerNames()\n",
    "\n",
    "# Check point\n",
    "#print(layers_all)\n",
    "\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "# Check point\n",
    "print()\n",
    "#print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7484d",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e5ad776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "PATH = \"./tl_detection.pth\"\n",
    "model = torchvision.models.resnet34(weights = torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model.fc = torch.nn.Sequential(\n",
    "    model.fc,\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 512, True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 43, True),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "            \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29416f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(43, 3)\n",
      "[ 17 142  97]\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability to eliminate weak detections\n",
    "probability_minimum = 0.2\n",
    "\n",
    "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "threshold = 0.2\n",
    "\n",
    "# Generating colours for bounding boxes\n",
    "# randint(low, high=None, size=None, dtype='l')\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Check point\n",
    "print(type(colours))  # <class 'numpy.ndarray'>\n",
    "print(colours.shape)  # (43, 3)\n",
    "print(colours[0])  # [25  65 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "76e3679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 230)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"./stop.jpeg\")\n",
    "#img = img.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
    "#resized_image = cv2.resize(img, (640, 640))\n",
    "img2 = img.copy()\n",
    "img2 = np.array(img2)\n",
    "h, w = img.shape[:2]\n",
    "h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b9f7ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "     transforms.Resize((224, 224), antialias=True)\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "16c781eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(img,1,(640, 640),[ 0,0,0], crop=False, swapRB=True)\n",
    "network.setInput(blob)\n",
    "output_from_network = network.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e04de78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_from_network\n",
    "bounding_boxes = []\n",
    "confidences = []\n",
    "class_numbers = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1503af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop\n",
      "14\n",
      "[94, 205, 35]\n",
      "(219, 230, 3)\n",
      "(-669, -6505) (9959, 14658)\n",
      "Stop\n",
      "14\n",
      "[94, 205, 35]\n",
      "(219, 230, 3)\n",
      "(-10829, -31737) (33428, 55672)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Going through all output layers after feed forward pass\n",
    "for result in output_from_network:\n",
    "    # Going through all detections from current output layer\n",
    "    for detected_objects in result:\n",
    "        # Getting 80 classes' probabilities for current detected object\n",
    "        scores = detected_objects[5:]\n",
    "        # Getting index of the class with the maximum value of probability\n",
    "        class_current = np.argmax(scores)\n",
    "        # Getting value of probability for defined class\n",
    "        confidence_current = scores[class_current]\n",
    "\n",
    "        # Eliminating weak predictions by minimum probability\n",
    "        if confidence_current > probability_minimum:\n",
    "            # Scaling bounding box coordinates to the initial frame size\n",
    "            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "            #print(box_current)\n",
    "            # Getting top left corner coordinates\n",
    "            x_center, y_center, box_width, box_height = box_current\n",
    "            x_min = int(x_center - (box_width / 2))\n",
    "            y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "            # Adding results into prepared lists\n",
    "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "            confidences.append(float(confidence_current))\n",
    "            class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "# Implementing non-maximum suppression of given bounding boxes\n",
    "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "#print(results)\n",
    "# Checking if there is any detected object been left\n",
    "if len(results) > 0:\n",
    "    # Going through indexes of results\n",
    "    for i in results.flatten():\n",
    "        # Bounding box coordinates, its width and height\n",
    "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "        # Cut fragment with Traffic Sign\n",
    "        c_ts = img[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "        # print(c_ts.shape)\n",
    "            \n",
    "        if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "            pass\n",
    "        else:\n",
    "            # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "            blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "            blob_ts[0] = blob_ts[0, :, :, :] #- mean['mean_image_rgb']\n",
    "            blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "            # plt.imshow(blob_ts[0, :, :, :])\n",
    "            #plt.show()\n",
    "            blob_ts = np.squeeze(blob_ts)\n",
    "            #print(blob_ts.shape)\n",
    "            blob_ts = transform(blob_ts)\n",
    "            #print(blob_ts.shape)\n",
    "           \n",
    "            # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "            scores = model(blob_ts.unsqueeze(0))\n",
    "\n",
    "            # Scores is given for image with 43 numbers of predictions for each class\n",
    "            # Getting only one class with maximum value\n",
    "            prediction = torch.argmax(scores).cpu().numpy()\n",
    "            print(labels['SignName'][prediction])\n",
    "            print(prediction)\n",
    "            \n",
    "            # Colour for current bounding box\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "            print(colour_box_current)\n",
    "            # Green BGR\n",
    "            #colour_box_current = [0, 255, 61]\n",
    "            \n",
    "            # Yellow BGR\n",
    "#             colour_box_current = [0, 255, 255]\n",
    "            print(img2.shape)\n",
    "            #print(img2[:,::-1].shape)\n",
    "            #print(img2)\n",
    "            # Drawing bounding box on the original current frame\n",
    "            cv2.rectangle(img2, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              (0,0,0),  thickness=2)\n",
    "            \n",
    "            print((x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height))\n",
    "            \n",
    "# Saving image\n",
    "cv2.imwrite('result.png', img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464cfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
